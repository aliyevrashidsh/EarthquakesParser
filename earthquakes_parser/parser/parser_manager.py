"""Main parser manager orchestrating the parsing workflow."""

from typing import Optional
from urllib.parse import urlparse

from earthquakes_parser.search import GoogleSearcher, DDGSearcher
from earthquakes_parser.storage.supabase.database import SupabaseDB
from earthquakes_parser.parser.data_extractor import DataExtractor
from earthquakes_parser.parser.models import ParsedContent
from earthquakes_parser.parser.schema_extractor import SchemaExtractor
from earthquakes_parser.parser.schema_manager import SchemaManager
from earthquakes_parser.storage.supabase.file_storage import SupabaseFileStorage
from earthquakes_parser.search.search_manager import SearchManager


class ParserManager:
    """Orchestrates the parsing workflow for search results."""

    def __init__(
        self,
        db: SupabaseDB,
        file_storage: SupabaseFileStorage,
        openai_base_url: str = "http://192.168.8.22:9999/v1",
        openai_api_key: str = "api-key",
    ):
        """Initialize parser manager.

        Args:
            db: Database instance.
            file_storage: File storage instance for HTML files.
            openai_base_url: Base URL for OpenAI-compatible API.
            openai_api_key: API key for OpenAI.
        """
        self.db = db
        self.file_storage = file_storage
        self.searcher = DDGSearcher()
        self.search_manager = SearchManager(self.db, self.searcher)
        self.schema_manager = SchemaManager(db)
        self.schema_extractor = SchemaExtractor(openai_base_url, openai_api_key)
        self.data_extractor = DataExtractor()

        self.parsed_content_table = "parsed_content"

    def _get_domain(self, url: str) -> str:
        """Extract domain from URL.

        Args:
            url: Full URL.

        Returns:
            Domain name (e.g., 'example.com').
        """
        parsed = urlparse(url)
        return parsed.netloc

    def _save_parsed_content(
        self,
        search_result_id: str,
        main_text: list,
        date: Optional[str],
        page_schema_id: str,
    ) -> Optional[str]:
        """Save parsed content to database.

        Args:
            search_result_id: ID of search result.
            main_text: Extracted main text.
            date: Extracted date.
            page_schema_id: Schema ID used for extraction.

        Returns:
            Parsed content ID or None if failed.
        """
        content = ParsedContent(
            search_result_id=search_result_id,
            main_text=main_text,
            date=date,
            page_schema_id=page_schema_id,
        )

        try:
            inserted_ids = self.db.insert(
                self.parsed_content_table,
                [content.to_dict()],
                batch_size=1,
            )
            return inserted_ids[0] if inserted_ids else None
        except Exception as e:
            print(f"âŒ Error saving parsed content: {e}")
            return None
#Ğ¿Ğ°Ñ€ÑĞ¸Ñ‚ Ğ¾Ğ´Ğ½Ñƒ Ğ·Ğ°Ğ¿Ğ¸ÑÑŒ
    def parse_record(
        self,
        record: dict,
        force_reextract: bool = False,
    ) -> bool:
        """Parse a single record from search_results.

        Args:
            record: Dict with keys: id, link, title, html_storage_path.
            force_reextract: Force schema re-extraction even if exists.

        Returns:
            True if successful, False otherwise.
        """
        search_result_id = str(record["id"])
        url = record["link"]
        title = record["title"]
        html_storage_path = record.get("html_storage_path")

        domain = self._get_domain(url)

        print(f"\n{'='*100}")
        print(f"ğŸŒ Processing: {title}")
        print(f"ğŸ”— URL: {url}")
        print(f"ğŸ“ Domain: {domain}")
        print(f"ğŸ“ Storage path: {html_storage_path}")

        # Step 1: Download HTML from storage
        if not html_storage_path:
            print(f"âŒ No HTML storage path for this record")
            self.search_manager.mark_as(search_result_id, "failed")
            return False

        html = self.file_storage.download(html_storage_path)
        if not html:
            print(f"âŒ Failed to download HTML from storage")
            self.search_manager.mark_as(search_result_id, "failed")
            return False

        print(f"âœ… HTML downloaded ({len(html)} characters)")

        # Step 2: Check if schema exists
        schema = self.schema_manager.get_by_domain(domain)

        if not schema or force_reextract:
            print(f"ğŸ“ No schema found for {domain}, extracting...")
            schema = self.schema_extractor.extract_schema(html, title, domain)

            if not schema:
                print(f"âŒ Failed to extract schema for {domain}")
                self.search_manager.mark_as(search_result_id, "failed")
                return False

            # Save schema
            schema_id = self.schema_manager.save(schema)
            if not schema_id:
                print(f"âŒ Failed to save schema for {domain}")
                self.search_manager.mark_as(search_result_id, "failed")
                return False

            schema.id = schema_id
            print(f"âœ… Schema saved with ID: {schema_id}")
        else:
            print(f"âœ… Using existing schema (ID: {schema.id})")

        # Check if page is valid
        if not schema.is_valid:
            print(f"âš ï¸ Page is not about earthquakes, skipping...")
            self.search_manager.mark_as(search_result_id, "failed")
            return False

        # Step 3: Extract data
        result = self.data_extractor.extract(html, schema)

        # Step 4: Check if extraction was successful
        # Failed only if main_text is empty (date can be None)
        if not result.main_text:
            print(f"âš ï¸ Extraction failed (main_text empty), re-extracting schema...")

            # Re-extract schema
            schema = self.schema_extractor.extract_schema(html, title, domain)
            if not schema:
                print(f"âŒ Failed to re-extract schema")
                self.search_manager.mark_as(search_result_id, "failed")
                return False

            # Save updated schema
            schema_id = self.schema_manager.save(schema)
            if not schema_id:
                print(f"âŒ Failed to save re-extracted schema")
                self.search_manager.mark_as(search_result_id, "failed")
                return False

            schema.id = schema_id

            # Try extraction again
            result = self.data_extractor.extract(html, schema)

            if not result.main_text:
                print(f"âŒ Re-extraction also failed, marking as failed...")
                self.search_manager.mark_as(search_result_id, "failed")
                return False

        # Step 5: Save parsed content
        print(f"ğŸ“Œ Extracted text: {len(result.main_text)} paragraphs")
        print(f"ğŸ“… Date: {result.date or 'Not found (OK)'}")

        content_id = self._save_parsed_content(
            search_result_id=search_result_id,
            main_text=result.main_text,
            date=result.date,
            page_schema_id=schema.id,
        )

        if content_id:
            print(f"âœ… Content saved with ID: {content_id}")
            # Mark as parsed
            self.search_manager.mark_as(search_result_id, "parsed")
            return True
        else:
            print(f"âŒ Failed to save content")
            self.search_manager.mark_as(search_result_id, "failed")
            return False
#Ğ¿Ğ°Ñ€ÑĞ¸Ñ‚ Ğ²ÑĞµ downloaded Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸
    def parse_downloaded(self, limit: Optional[int] = None) -> dict:
        """Parse all downloaded URLs from search_results table.

        Args:
            limit: Maximum number of records to process.

        Returns:
            Dictionary with statistics.
        """
        print(f"ğŸ“¥ Loading downloaded search results from database...")

        # Get records with status='downloaded'
        records = self.search_manager.get_urls(status="downloaded", limit=limit or 100)

        if not records:
            print("âš ï¸ No downloaded search results found in database")
            return {"total": 0, "successful": 0, "failed": 0}

        print(f"âœ… Loaded {len(records)} downloaded results")

        stats = {
            "total": len(records),
            "successful": 0,
            "failed": 0,
        }

        for record in records:
            try:
                success = self.parse_record(record)

                if success:
                    stats["successful"] += 1
                else:
                    stats["failed"] += 1

            except Exception as e:
                print(f"âŒ Error processing record: {e}")
                stats["failed"] += 1

                # Mark as failed in DB
                try:
                    self.search_manager.mark_as(str(record["id"]), "failed")
                except:
                    pass

        print(f"\n{'='*100}")
        print(f"ğŸ“Š Parsing complete:")
        print(f"   Total: {stats['total']}")
        print(f"   âœ… Successful: {stats['successful']}")
        print(f"   âŒ Failed: {stats['failed']}")
        print(f"{'='*100}")

        return stats

    def get_statistics(self) -> dict:
        """Get parsing statistics from database.

        Returns:
            Dictionary with statistics for each status.
        """
        return self.search_manager.get_statistics()