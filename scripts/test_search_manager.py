"""Test SearchManager business logic with Supabase."""

import os
import tempfile

from dotenv import load_dotenv

# Load environment
load_dotenv()

from earthquakes_parser import SearchManager, SupabaseDB

print("=" * 60)
print("Test SearchManager Business Logic")
print("=" * 60)

# Initialize utilities
db = SupabaseDB()
search_manager = SearchManager(db)

print("\nâœ“ SearchManager initialized")
print("âœ“ Using SupabaseDB for persistence")

# Test 1: Search and save with deduplication
print("\n" + "=" * 60)
print("Test 1: Search and Save with Deduplication")
print("=" * 60)

keywords = ["Ğ·ĞµĞ¼Ğ»ĞµÑ‚Ñ€ÑÑĞµĞ½Ğ¸Ğµ ĞĞ»Ğ¼Ğ°Ñ‚Ñ‹", "earthquake Kazakhstan"]

print(f"\nâ³ Searching for keywords: {keywords}")
print("   Max results: 3 per keyword")
print("   Deduplication: enabled")

stats = search_manager.search_and_save(
    keywords=keywords, max_results=3, skip_existing=True
)

print(f"\nğŸ“Š Results:")
print(f"   âœ“ Keywords searched: {stats['searched']}")
print(f"   âœ“ Total results found: {stats['found']}")
print(f"   âœ“ New results saved: {stats['new']}")
print(f"   âœ“ Existing skipped: {stats['skipped']}")

# Test 2: Get pending URLs
print("\n" + "=" * 60)
print("Test 2: Get Pending URLs for Download")
print("=" * 60)

print("\nâ³ Fetching pending URLs...")
pending_urls = search_manager.get_pending_urls(limit=5)

print(f"\nâœ“ Found {len(pending_urls)} pending URLs")
if pending_urls:
    print("\nFirst 3 pending URLs:")
    for i, url_data in enumerate(pending_urls[:3], 1):
        print(f"   {i}. {url_data['title']}")
        print(f"      URL: {url_data['link']}")
        print(f"      Query: {url_data['query']}")
        print(f"      Status: {url_data['status']}")

# Test 3: Mark as downloaded
if pending_urls:
    print("\n" + "=" * 60)
    print("Test 3: Mark URL as Downloaded")
    print("=" * 60)

    test_result = pending_urls[0]
    print(f"\nâ³ Marking as downloaded: {test_result['title']}")

    success = search_manager.mark_as_downloaded(
        test_result["id"], html_storage_path="html/test_file.html"
    )

    if success:
        print("âœ“ Successfully marked as downloaded")
    else:
        print("âœ— Failed to mark as downloaded")

# Test 4: Get statistics
print("\n" + "=" * 60)
print("Test 4: Get Search Statistics")
print("=" * 60)

print("\nâ³ Fetching statistics...")
stats = search_manager.get_statistics()

print("\nğŸ“Š Database Statistics:")
print(f"   Total records: {stats['total']}")
print(f"   Pending: {stats['pending']}")
print(f"   Downloaded: {stats['downloaded']}")
print(f"   Parsed: {stats['parsed']}")
print(f"   Analyzed: {stats['analyzed']}")
print(f"   Failed: {stats['failed']}")

# Test 5: Search from keywords file
print("\n" + "=" * 60)
print("Test 5: Search with Keywords File")
print("=" * 60)

# Create temporary keywords file
with tempfile.NamedTemporaryFile(
    mode="w", suffix=".txt", delete=False, encoding="utf-8"
) as f:
    f.write("Ğ·ĞµĞ¼Ğ»ĞµÑ‚Ñ€ÑÑĞµĞ½Ğ¸Ğµ\n")
    f.write("ÑĞµĞ¹ÑĞ¼Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ\n")
    keywords_file = f.name

print(f"\nâ³ Created test keywords file: {keywords_file}")
print("   Keywords: Ğ·ĞµĞ¼Ğ»ĞµÑ‚Ñ€ÑÑĞµĞ½Ğ¸Ğµ, ÑĞµĞ¹ÑĞ¼Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ")

stats = search_manager.search_with_keywords_file(
    keywords_file, max_results=2, skip_existing=True
)

print(f"\nğŸ“Š Results:")
print(f"   âœ“ Keywords searched: {stats['searched']}")
print(f"   âœ“ Total results found: {stats['found']}")
print(f"   âœ“ New results saved: {stats['new']}")
print(f"   âœ“ Existing skipped: {stats['skipped']}")

# Clean up
os.remove(keywords_file)
print(f"\nâœ“ Cleaned up test file")

# Final summary
print("\n" + "=" * 60)
print("âœ… All SearchManager tests completed!")
print("=" * 60)

print("\nğŸ“ SearchManager Features Demonstrated:")
print("   1. âœ… Search and save with automatic deduplication")
print("   2. âœ… Get pending URLs for download pipeline")
print("   3. âœ… Mark URLs as downloaded (status management)")
print("   4. âœ… Get comprehensive statistics")
print("   5. âœ… Search from keywords file")

print("\nğŸ¯ Business Logic Benefits:")
print("   â€¢ Automatic deduplication prevents duplicate URLs")
print("   â€¢ Status tracking enables pipeline workflow")
print("   â€¢ Statistics provide visibility into data")
print("   â€¢ Reusable methods for different workflows")
